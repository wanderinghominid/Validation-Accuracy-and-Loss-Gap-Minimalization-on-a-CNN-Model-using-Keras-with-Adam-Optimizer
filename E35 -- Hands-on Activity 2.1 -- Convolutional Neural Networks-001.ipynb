{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcAeyMmfV9Gh"
   },
   "source": [
    "### ASSIGNMENT: \n",
    "PSMDSSE 102-PSMDS2 - Deep Learning.\n",
    "\n",
    "This activity aims to introduce how to build a convolutional neural network; detecting images with MASK and NO_MASK using CNN.\n",
    "\n",
    "STUDENT: CIRILO P TRADIO JR | TIP - MANILA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYewVPjJ0ClA"
   },
   "source": [
    "# **Convolutional Neural Network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QRszcgT0Nw8"
   },
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kdu-ogR80q5R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXQXr-G10T9n"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwlNWgTq2RMC"
   },
   "source": [
    "Pre-processing Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPMcLQIW0uNT",
    "outputId": "f7cdf9b9-ea40-4f6b-ff41-affe5cc1b013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 460 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(r'C:\\Users\\offic\\Dropbox\\My PC (LAPTOP-5P66QLDI)\\Desktop\\MASK-NO_MASK\\Training_Set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 10,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fw0kCvg2zpR"
   },
   "source": [
    "Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdesAYJv20f3",
    "outputId": "c83dcccd-70b2-423c-e4e6-da2858dfc605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(r'C:\\Users\\offic\\Dropbox\\My PC (LAPTOP-5P66QLDI)\\Desktop\\MASK-NO_MASK\\Testing_Set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 10,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_set = val_datagen.flow_from_directory(r'C:\\Users\\offic\\Dropbox\\My PC (LAPTOP-5P66QLDI)\\Desktop\\MASK-NO_MASK\\Validation_Set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 10,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a7pIG8l0YRJ"
   },
   "source": [
    "## Part 2 - Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0308okb3SOx"
   },
   "source": [
    "#### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4ZD07A-B3UGG"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F0djChB3oDP"
   },
   "source": [
    "### Step 1 - Adding Convolutional Layer and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ce-0Dg1q3ltw"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', \n",
    "                               input_shape = (64,64, 3)))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPFzTAoR07Uf"
   },
   "source": [
    "### Step 2 - Flattening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m9ENFuXK3ydf"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo4RXIs01BIk"
   },
   "source": [
    "### Step 3 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LC7Y2JU234dt"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=16, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RBjkhHQ1E51"
   },
   "source": [
    "### Step 4 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2e2OfDvi37pF"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Checking Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 109,345\n",
      "Trainable params: 109,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F0jX4QO0d0U"
   },
   "source": [
    "## **Part 3 - Training CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Lt7_r_Lz0wSA"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8MNJQp74GMS"
   },
   "source": [
    "#### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pe1pp7xq4HeR",
    "outputId": "c5af1c59-5025-49b5-e557-bc74695272ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.0306 - accuracy: 0.9913 - val_loss: 0.2317 - val_accuracy: 0.9052\n",
      "Epoch 2/40\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 0.3161 - val_accuracy: 0.8966\n",
      "Epoch 3/40\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.0311 - accuracy: 0.9913 - val_loss: 0.2777 - val_accuracy: 0.9138\n",
      "Epoch 4/40\n",
      "46/46 [==============================] - 8s 180ms/step - loss: 0.0709 - accuracy: 0.9717 - val_loss: 0.3026 - val_accuracy: 0.8879\n",
      "Epoch 5/40\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.2737 - val_accuracy: 0.9224\n",
      "Epoch 6/40\n",
      "46/46 [==============================] - 8s 184ms/step - loss: 0.0129 - accuracy: 0.9978 - val_loss: 0.3956 - val_accuracy: 0.8793\n",
      "Epoch 7/40\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5548 - val_accuracy: 0.8879\n",
      "Epoch 8/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0792 - accuracy: 0.9761 - val_loss: 0.3272 - val_accuracy: 0.9052\n",
      "Epoch 9/40\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.0797 - accuracy: 0.9674 - val_loss: 0.2834 - val_accuracy: 0.9138\n",
      "Epoch 10/40\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.0815 - accuracy: 0.9652 - val_loss: 0.3022 - val_accuracy: 0.8966\n",
      "Epoch 11/40\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.0805 - accuracy: 0.9674 - val_loss: 0.3570 - val_accuracy: 0.8879\n",
      "Epoch 12/40\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.0525 - accuracy: 0.9783 - val_loss: 0.4289 - val_accuracy: 0.8793\n",
      "Epoch 13/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0317 - accuracy: 0.9891 - val_loss: 0.6701 - val_accuracy: 0.8362\n",
      "Epoch 14/40\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.0684 - accuracy: 0.9848 - val_loss: 0.3609 - val_accuracy: 0.8966\n",
      "Epoch 15/40\n",
      "46/46 [==============================] - 8s 178ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 0.3935 - val_accuracy: 0.8966\n",
      "Epoch 16/40\n",
      "46/46 [==============================] - 8s 160ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 0.3280 - val_accuracy: 0.9052\n",
      "Epoch 17/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0105 - accuracy: 0.9957 - val_loss: 0.4250 - val_accuracy: 0.9052\n",
      "Epoch 18/40\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.0133 - accuracy: 0.9978 - val_loss: 0.3981 - val_accuracy: 0.8966\n",
      "Epoch 19/40\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.0796 - accuracy: 0.9739 - val_loss: 0.4727 - val_accuracy: 0.8707\n",
      "Epoch 20/40\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.3020 - val_accuracy: 0.9224\n",
      "Epoch 21/40\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.0374 - accuracy: 0.9826 - val_loss: 0.4302 - val_accuracy: 0.9138\n",
      "Epoch 22/40\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.4379 - val_accuracy: 0.9052\n",
      "Epoch 23/40\n",
      "46/46 [==============================] - 8s 183ms/step - loss: 0.0894 - accuracy: 0.9674 - val_loss: 0.2546 - val_accuracy: 0.8879\n",
      "Epoch 24/40\n",
      "46/46 [==============================] - 8s 181ms/step - loss: 0.0783 - accuracy: 0.9739 - val_loss: 0.2313 - val_accuracy: 0.9310\n",
      "Epoch 25/40\n",
      "46/46 [==============================] - 8s 175ms/step - loss: 0.0404 - accuracy: 0.9826 - val_loss: 0.3871 - val_accuracy: 0.9224\n",
      "Epoch 26/40\n",
      "46/46 [==============================] - 8s 178ms/step - loss: 0.0528 - accuracy: 0.9761 - val_loss: 0.4293 - val_accuracy: 0.9052\n",
      "Epoch 27/40\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.3841 - val_accuracy: 0.9052\n",
      "Epoch 28/40\n",
      "46/46 [==============================] - 8s 182ms/step - loss: 0.0484 - accuracy: 0.9870 - val_loss: 0.3169 - val_accuracy: 0.9224\n",
      "Epoch 29/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.4243 - val_accuracy: 0.9052\n",
      "Epoch 30/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.4421 - val_accuracy: 0.9224\n",
      "Epoch 31/40\n",
      "46/46 [==============================] - 8s 175ms/step - loss: 0.0113 - accuracy: 0.9957 - val_loss: 0.4736 - val_accuracy: 0.9138\n",
      "Epoch 32/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.5022 - val_accuracy: 0.9138\n",
      "Epoch 33/40\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.0323 - accuracy: 0.9870 - val_loss: 0.3754 - val_accuracy: 0.9052\n",
      "Epoch 34/40\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.0117 - accuracy: 0.9978 - val_loss: 0.5649 - val_accuracy: 0.8793\n",
      "Epoch 35/40\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.0335 - accuracy: 0.9935 - val_loss: 0.4919 - val_accuracy: 0.9052\n",
      "Epoch 36/40\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0126 - accuracy: 0.9978 - val_loss: 0.4686 - val_accuracy: 0.9052\n",
      "Epoch 37/40\n",
      "46/46 [==============================] - 8s 175ms/step - loss: 0.0084 - accuracy: 0.9957 - val_loss: 0.5974 - val_accuracy: 0.8879\n",
      "Epoch 38/40\n",
      "46/46 [==============================] - 8s 167ms/step - loss: 0.0669 - accuracy: 0.9804 - val_loss: 0.3624 - val_accuracy: 0.9138\n",
      "Epoch 39/40\n",
      "46/46 [==============================] - 8s 177ms/step - loss: 0.0181 - accuracy: 0.9913 - val_loss: 0.4404 - val_accuracy: 0.9052\n",
      "Epoch 40/40\n",
      "46/46 [==============================] - 8s 175ms/step - loss: 0.0613 - accuracy: 0.9826 - val_loss: 0.3311 - val_accuracy: 0.8966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb1a698550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 18ms/step - loss: 0.2660 - accuracy: 0.8966\n",
      "The testing accuracy is: 89.66% %\n"
     ]
    }
   ],
   "source": [
    "test_accu_1 = cnn.evaluate(test_set)\n",
    "print('The testing accuracy is:','{:.2f}%'.format(test_accu_1[1]*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 14ms/step - loss: 1.7283 - accuracy: 0.8500\n",
      "The testing accuracy is: 85.00% %\n"
     ]
    }
   ],
   "source": [
    "val_accu_1 = cnn.evaluate(val_set)\n",
    "print('The testing accuracy is:','{:.2f}%'.format(val_accu_1[1]*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MK937hQ0gog"
   },
   "source": [
    "## **Part 4 - Making Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THDo0RoM0w3k",
    "outputId": "aac428a1-d7e3-4f46-d9b0-1afa79d01777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00036886334\n",
      "The image has mask\n"
     ]
    }
   ],
   "source": [
    "#Testing Image\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'C:\\Users\\offic\\Dropbox\\My PC (LAPTOP-5P66QLDI)\\Desktop\\MASK-NO_MASK\\Single_Test\\with_mask_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "print(result[0][0])\n",
    "if result[0][0] > 0.5:\n",
    "  prediction = 'no_mask'\n",
    "else:\n",
    "  prediction = 'mask'\n",
    "print('The image has', prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CAT-DOG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
